import spacy
import re

# Assumption: the file is a plain text file with one tweet per line
tweets_file = "tweets.txt"
racial_slurs = {"slur1", "slur2", "slur3", ...} # Assumption: a set of racial slurs is provided

# Load the spacy NLP model
nlp = spacy.load("en_core_web_sm")

# Define a function to calculate the degree of profanity for a given sentence using NLP
def calculate_profanity(sentence):
    # Parse the sentence using the spacy NLP model
    doc = nlp(sentence)
    # Count the number of racial slurs in the sentence
    num_slurs = sum([1 for token in doc if token.text.lower() in racial_slurs])
    # Check for additional instances of racial slurs that may be misspelled or written with special characters
    additional_slurs = 0
    for slur in racial_slurs:
        regex = re.compile(re.escape(slur), re.IGNORECASE)
        additional_slurs += len(regex.findall(sentence))
    # Calculate the degree of profanity as a percentage of total words
    total_tokens = len(doc) + additional_slurs
    profanity_degree = num_slurs / total_tokens * 100 if total_tokens > 0 else 0
    return profanity_degree

# Read in the tweets file and calculate the degree of profanity for each sentence
with open(tweets_file, "r") as f:
    tweets = f.readlines()
    for tweet in tweets:
        profanity_degree = calculate_profanity(tweet.strip())
        print(f"Sentence: {tweet.strip()}, Profanity Degree: {profanity_degree:.2f}%")
